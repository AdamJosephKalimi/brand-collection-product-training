# Document Upload & Item Generation Pipeline

This document explains the two-stage process for creating collection items from purchase orders and line sheets.

---

## Overview

The system uses a **two-stage pipeline**:

1. **Document Upload** - Parse and extract structured data from documents
2. **Item Generation** - Combine PO and Line Sheet data to create final items

---

## Stage 1: Document Upload Pipeline

### Trigger
User uploads documents (PO + Line Sheet) and clicks **"Save Documents"**

### Process Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. UPLOAD TO FIREBASE STORAGE                               â”‚
â”‚    â€¢ PO: /collections/{id}/documents/purchase_order.xlsx    â”‚
â”‚    â€¢ Line Sheet: /collections/{id}/documents/line_sheet.pdf â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. CREATE FIRESTORE METADATA                                â”‚
â”‚    â€¢ Document ID, name, type, storage path                  â”‚
â”‚    â€¢ Initial fields: parsed_text, normalized_text = null    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. PARSE DOCUMENT CONTENT                                   â”‚
â”‚    â€¢ PDF: OCR + table detection â†’ raw text                  â”‚
â”‚    â€¢ Excel: Extract all rows/columns â†’ raw text             â”‚
â”‚    â€¢ Result: parsed_text field populated                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. NORMALIZE TEXT                                           â”‚
â”‚    â€¢ Clean up formatting, remove noise                      â”‚
â”‚    â€¢ Standardize whitespace and structure                   â”‚
â”‚    â€¢ Result: normalized_text field populated                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. EXTRACT STRUCTURED DATA (LINE SHEETS ONLY)               â”‚
â”‚    â€¢ Split normalized_text into 20K character chunks        â”‚
â”‚    â€¢ Send each chunk to LLM (GPT-4)                         â”‚
â”‚    â€¢ LLM extracts: SKU, name, price, color, material, etc.  â”‚
â”‚    â€¢ Deduplicate products across chunks                     â”‚
â”‚    â€¢ Result: structured_products field populated            â”‚
â”‚                                                              â”‚
â”‚    âš ï¸  Purchase Orders skip this step                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. SAVE TO FIRESTORE                                        â”‚
â”‚    â€¢ Update document with all extracted data                â”‚
â”‚    â€¢ Line Sheet: Has structured_products ready              â”‚
â”‚    â€¢ PO: Has parsed_text ready for later processing         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### What Gets Stored

#### Purchase Order Document
```json
{
  "document_id": "doc_abc123",
  "type": "purchase_order",
  "storage_path": "collections/.../purchase_order.xlsx",
  "parsed_text": "Raw Excel content as text",
  "normalized_text": "Cleaned text",
  "structured_products": null  â† Not extracted for PO
}
```

#### Line Sheet Document
```json
{
  "document_id": "doc_xyz789",
  "type": "line_sheet",
  "storage_path": "collections/.../line_sheet.pdf",
  "parsed_text": "Raw PDF content",
  "normalized_text": "Cleaned text",
  "structured_products": [  â† LLM EXTRACTED
    {
      "sku": "PA05669305",
      "product_name": "Baggy Jeans",
      "wholesale_price": 134,
      "rrp": 335,
      "color": "Medium Wash",
      "material": "100% Cotton",
      "origin": "United States"
    }
    // ... more products
  ]
}
```

### Key Points

- âœ… **Synchronous** - Upload waits for parsing to complete
- âœ… **Line sheets get LLM extraction** during upload
- âœ… **Purchase orders do NOT** get LLM extraction
- âœ… **Progressive updates** - Firestore updated as chunks are processed
- âœ… **Deduplication** - Products are deduplicated across chunks

---

## Stage 2: Item Generation Pipeline

### Trigger
User clicks **"ğŸš€ Generate Items"** button

### Process Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. FETCH PURCHASE ORDER DOCUMENT                            â”‚
â”‚    â€¢ Query Firestore for type = "purchase_order"            â”‚
â”‚    â€¢ Get document metadata and storage path                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. DOWNLOAD PO FILE FROM STORAGE                            â”‚
â”‚    â€¢ Download Excel file from Firebase Storage              â”‚
â”‚    â€¢ Load into memory as bytes                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. PARSE EXCEL FILE                                         â”‚
â”‚    â€¢ Extract raw headers and rows                           â”‚
â”‚    â€¢ No interpretation yet, just structure                  â”‚
â”‚    â€¢ Result: { headers: [...], rows: [[...], ...] }        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. IDENTIFY COLUMNS WITH LLM                                â”‚
â”‚    â€¢ Send headers + sample rows to GPT-4                    â”‚
â”‚    â€¢ LLM identifies which columns contain:                  â”‚
â”‚      - SKU                                                   â”‚
â”‚      - Color/Color Code                                     â”‚
â”‚      - Sizes (S, M, L, 30, 32, etc.)                        â”‚
â”‚      - Quantities per size                                  â”‚
â”‚    â€¢ Result: Column mapping                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. EXTRACT SKU DATA FROM PO                                 â”‚
â”‚    â€¢ Use column mapping to extract data                     â”‚
â”‚    â€¢ For each row, extract:                                 â”‚
â”‚      - Full SKU (e.g., "PA05669305BL139")                   â”‚
â”‚      - Base SKU (e.g., "PA05669305")                        â”‚
â”‚      - Color code (e.g., "BL139" or "4018")                 â”‚
â”‚      - Sizes and quantities (e.g., {"30": 10, "32": 20})    â”‚
â”‚    â€¢ Result: List of PO items with SKU + size data          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. ENRICH FROM LINE SHEET                                   â”‚
â”‚    â€¢ Fetch Line Sheet document from Firestore               â”‚
â”‚    â€¢ Get structured_products (already LLM extracted)        â”‚
â”‚    â€¢ For each PO item:                                      â”‚
â”‚      - Match by base SKU                                    â”‚
â”‚      - Add: product_name, prices, origin, material          â”‚
â”‚    â€¢ Result: Enriched items with complete data              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. GENERATE ITEM OBJECTS                                    â”‚
â”‚    â€¢ Create final item structure for each SKU/color         â”‚
â”‚    â€¢ Generate unique item_id                                â”‚
â”‚    â€¢ Generate content_hash (SKU + color_code)               â”‚
â”‚    â€¢ Add timestamps, metadata                               â”‚
â”‚    â€¢ Result: Complete item objects ready for Firestore      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. SAVE TO FIRESTORE WITH DUPLICATE DETECTION               â”‚
â”‚    â€¢ For each item:                                         â”‚
â”‚      - Check if content_hash already exists                 â”‚
â”‚      - If exists: Skip (duplicate)                          â”‚
â”‚      - If new: Save to /collections/{id}/items/{item_id}    â”‚
â”‚    â€¢ Update collection stats                                â”‚
â”‚    â€¢ Result: Items saved, stats returned                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Data Flow

#### Input: PO Row
```
SKU: PA05669305BL139
Size 30: 10
Size 32: 20
Size 34: 15
```

#### Input: Line Sheet Product
```json
{
  "sku": "PA05669305",
  "product_name": "Baggy Jeans",
  "wholesale_price": 134,
  "rrp": 335,
  "color": "Medium Wash",
  "material": "100% Cotton",
  "origin": "United States"
}
```

#### Output: Final Item
```json
{
  "item_id": "item_a1b2c3d4e5f6g7h8",
  "collection_id": "coll_xyz789",
  "content_hash": "a1b2c3d4e5f6g7h8",
  
  // From PO
  "sku": "PA05669305BL139",
  "base_sku": "PA05669305",
  "color_code": "BL139",
  "sizes": {
    "30": 10,
    "32": 20,
    "34": 15
  },
  
  // From Line Sheet
  "product_name": "Baggy Jeans",
  "color": "Medium Wash",
  "wholesale_price": 134,
  "rrp": 335,
  "material": ["100% Cotton"],
  "origin": "United States",
  
  // Metadata
  "category": null,
  "subcategory": null,
  "highlighted_item": false,
  "source_documents": {
    "purchase_order_id": "doc_abc123",
    "line_sheet_id": "doc_xyz789"
  },
  "created_at": "2025-10-23T20:00:00Z",
  "updated_at": "2025-10-23T20:00:00Z"
}
```

### Duplicate Detection

Items are deduplicated using a **content hash** based on:
- SKU
- Color code

If you run "Generate Items" multiple times:
- âœ… Existing items are skipped
- âœ… Only new items are created
- âœ… Stats show: `items_created` vs `items_skipped`

### Key Points

- âœ… **Separate from upload** - Must click "Generate Items"
- âœ… **Combines two sources** - PO (sizes/quantities) + Line Sheet (product details)
- âœ… **LLM identifies columns** - Handles different PO formats
- âœ… **Duplicate detection** - Safe to re-run
- âœ… **Returns statistics** - Shows what was created/skipped

---

## Summary

### Document Upload
**Purpose:** Parse and extract structured data from documents

**What happens:**
- Files uploaded to Storage
- Text extracted and normalized
- **Line sheets get LLM extraction** â†’ `structured_products`
- Purchase orders stored as-is for later processing

**Result:** Documents ready for item generation

### Item Generation
**Purpose:** Create final items by combining PO and Line Sheet data

**What happens:**
- PO parsed for SKUs, sizes, quantities
- Line Sheet `structured_products` fetched (already extracted)
- Data matched by base SKU
- Final items created and saved

**Result:** Items in `/collections/{id}/items` ready for display

---

## File Locations

### Backend Services
- **Document Upload:** `backend/app/services/collection_document_service.py`
- **Item Generation:** `backend/app/services/item_generation_service.py`

### API Endpoints
- **Upload Document:** `POST /api/collections/{id}/documents`
- **Generate Items:** `POST /api/collections/{id}/items/generate`
- **Get Items:** `GET /api/collections/{id}/items`

### Frontend
- **UI Component:** `frontend/src/components/DocumentProcessingForm.js`
- **Document Upload:** `saveDocuments()` function
- **Item Generation:** `generateItems()` function

---

## Common Questions

### Q: Why two separate steps?
**A:** Document upload is generic (works for any document type). Item generation is specific to PO + Line Sheet matching logic.

### Q: Can I upload just a Line Sheet?
**A:** Yes, but you won't be able to generate items without a Purchase Order (need size/quantity data).

### Q: What if I upload a new PO?
**A:** Click "Generate Items" again. Duplicate detection will skip existing items and only create new ones.

### Q: Where does the LLM get called?
**A:** Twice:
1. During Line Sheet upload (extract products)
2. During Item Generation (identify PO columns)

### Q: How long does it take?
**A:** 
- Document Upload: 30-60 seconds (depends on document size)
- Item Generation: 10-30 seconds (depends on number of SKUs)
